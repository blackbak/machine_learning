{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "from util import relu, error_rate, getKaggleMNIST, init_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RBM(object):\n",
    "    def __init__(self, M, an_id):\n",
    "        self.M = M\n",
    "        self.id = an_id\n",
    "        self.rng = RandomStreams()\n",
    "    \n",
    "    def fit(self, X, learning_rate=0.1, epochs=10, batch_sz=100, show_fig=False):\n",
    "        N, D = X.shape\n",
    "        n_batches = int(N/batch_sz)\n",
    "        \n",
    "        W0 = init_weights((D, self.M))\n",
    "        self.W = theano.shared(W0, 'W_%s' %self.id)\n",
    "        self.c = theano.shared(np.zeros(self.M), 'c_%s' %self.id)\n",
    "        self.b = theano.shared(np.zeros(D), 'b_%s' %self.id)\n",
    "        self.params = [self.W, self.c, self.b]\n",
    "        self.forward_params = [self.W, self.c]\n",
    "        \n",
    "        self.dW = theano.shared(np.zeros(W0.shape), 'dW_%s' %self.id )\n",
    "        self.dc = theano.shared(np.zeros(self.M), 'dbh_%s' %self.id)\n",
    "        self.db = theano.shared(np.zeros(D), 'db0_%s' %self.id)\n",
    "        self.dparams = [self.dW, self.dc, self.db]\n",
    "        self.forward_dparams = [self.dW, self.dc]\n",
    "        \n",
    "        X_in = T.matrix('X_%s' %self.id)\n",
    "        H = T.nnet.sigmoid(X_in.dot(self.W)+self.c)\n",
    "        self.hidden_op = theano.function(\n",
    "            inputs=[X_in],\n",
    "            outputs=H\n",
    "        )\n",
    "        \n",
    "        X_hat = self.forward_output(X_in)\n",
    "        cost = -(X_in*T.log(X_hat)+(1-X_in)*T.log(1-X_hat)).sum()/N\n",
    "        cost_op = theano.function(\n",
    "            inputs=[X_in],\n",
    "            outputs=cost\n",
    "        )\n",
    "        \n",
    "        H_sample = self.sample_h_given_v(X_in)\n",
    "        X_sample = self.sample_v_given_h(H_sample)\n",
    "        \n",
    "        objective = T.mean(self.free_energy(X_in)) - T.mean(self.free_energy(X_sample))\n",
    "        \n",
    "        updates = [(p, p - learning_rate*T.grad(objective, p, consider_constant=[X_sample])) for p in self.params]\n",
    "        train_op = theano.function(\n",
    "            inputs = [X_in],\n",
    "            updates=updates\n",
    "        )\n",
    "        \n",
    "        costs=[]\n",
    "        print('training rbm %s' %self.id)\n",
    "        for i in range(epochs):\n",
    "            print('epoch:', i)\n",
    "            X = shuffle(X)\n",
    "            for j in range(n_batches):\n",
    "                batch = X[j*batch_sz:(j+1)*batch_sz]\n",
    "                train_op(batch)\n",
    "                the_cost = cost_op(X)\n",
    "                print(\"batch\", j, \"/\", n_batches, \"cost:\", the_cost)\n",
    "                costs.append(the_cost)\n",
    "            if show_fig:\n",
    "                plt.plot(costs)\n",
    "                plt.show()\n",
    "            \n",
    "    def free_energy(self, V):\n",
    "        return -V.dot(self.b) - T.sum(T.log(1+T.exp(V.dot(self.W)+self.c)), axis=1)\n",
    "    \n",
    "    def sample_h_given_v(self, V):\n",
    "        p_h_given_v = T.nnet.sigmoid(V.dot(self.W)+self.c)\n",
    "        h_sample = self.rng.binomial(size=p_h_given_v.shape, n=1, p=p_h_given_v)\n",
    "        return h_sample\n",
    "    \n",
    "    def sample_v_given_h(self, H):\n",
    "        p_v_given_h = T.nnet.sigmoid(H.dot(self.W.T) +self.b)\n",
    "        v_sample = self.rng.binomial(size=p_v_given_h.shape, n=1, p=p_v_given_h)\n",
    "        return v_sample\n",
    "    \n",
    "    def forward_hidden(self, X):\n",
    "        return T.nnet.sigmoid(X.dot(self.W)+self.c)\n",
    "    \n",
    "    def forward_output(self, X):\n",
    "        Z = self.forward_hidden(X)\n",
    "        Y = T.nnet.sigmoid(Z.dot(self.W.T)+self.b)\n",
    "        return Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DNN(object):\n",
    "    def __init__(self, hidden_layer_sizes, UnsupervisedModel=RBM):\n",
    "        self.hidden_layers = []\n",
    "        count = 0\n",
    "        for M in hidden_layer_sizes:\n",
    "            ae = UnsupervisedModel(M, count)\n",
    "            self.hidden_layers.append(ae)\n",
    "            count += 1\n",
    "\n",
    "\n",
    "    def fit(self, X, Y, Xtest, Ytest, pretrain=True, learning_rate=0.01, mu=0.99, reg=0.1, epochs=1, batch_sz=100):\n",
    "        # greedy layer-wise training of autoencoders\n",
    "        pretrain_epochs = 1\n",
    "        if not pretrain:\n",
    "            pretrain_epochs = 0\n",
    "\n",
    "        current_input = X\n",
    "        for ae in self.hidden_layers:\n",
    "            ae.fit(current_input, epochs=pretrain_epochs)\n",
    "\n",
    "            # create current_input for the next layer\n",
    "            current_input = ae.hidden_op(current_input)\n",
    "\n",
    "        # initialize logistic regression layer\n",
    "        N = len(Y)\n",
    "        K = len(set(Y))\n",
    "        W0 = init_weights((self.hidden_layers[-1].M, K))\n",
    "        self.W = theano.shared(W0, \"W_logreg\")\n",
    "        self.b = theano.shared(np.zeros(K), \"b_logreg\")\n",
    "\n",
    "        self.params = [self.W, self.b]\n",
    "        for ae in self.hidden_layers:\n",
    "            self.params += ae.forward_params\n",
    "\n",
    "        # for momentum\n",
    "        self.dW = theano.shared(np.zeros(W0.shape), \"dW_logreg\")\n",
    "        self.db = theano.shared(np.zeros(K), \"db_logreg\")\n",
    "        self.dparams = [self.dW, self.db]\n",
    "        for ae in self.hidden_layers:\n",
    "            self.dparams += ae.forward_dparams\n",
    "\n",
    "        X_in = T.matrix('X_in')\n",
    "        targets = T.ivector('Targets')\n",
    "        pY = self.forward(X_in)\n",
    "\n",
    "        # squared_magnitude = [(p*p).sum() for p in self.params]\n",
    "        # reg_cost = T.sum(squared_magnitude)\n",
    "        cost = -T.mean( T.log(pY[T.arange(pY.shape[0]), targets]) ) #+ reg*reg_cost\n",
    "        prediction = self.predict(X_in)\n",
    "        cost_predict_op = theano.function(\n",
    "            inputs=[X_in, targets],\n",
    "            outputs=[cost, prediction],\n",
    "        )\n",
    "\n",
    "        updates = [\n",
    "            (p, p + mu*dp - learning_rate*T.grad(cost, p)) for p, dp in zip(self.params, self.dparams)\n",
    "        ] + [\n",
    "            (dp, mu*dp - learning_rate*T.grad(cost, p)) for p, dp in zip(self.params, self.dparams)\n",
    "        ]\n",
    "        # updates = [(p, p - learning_rate*T.grad(cost, p)) for p in self.params]\n",
    "        train_op = theano.function(\n",
    "            inputs=[X_in, targets],\n",
    "            updates=updates,\n",
    "        )\n",
    "\n",
    "        n_batches = N / batch_sz\n",
    "        costs = []\n",
    "        print(\"supervised training...\")\n",
    "        for i in range(epochs):\n",
    "            print(\"epoch:\", i)\n",
    "            X, Y = shuffle(X, Y)\n",
    "            for j in range(n_batches):\n",
    "                Xbatch = X[j*batch_sz:(j*batch_sz + batch_sz)]\n",
    "                Ybatch = Y[j*batch_sz:(j*batch_sz + batch_sz)]\n",
    "                train_op(Xbatch, Ybatch)\n",
    "                the_cost, the_prediction = cost_predict_op(Xtest, Ytest)\n",
    "                error = error_rate(the_prediction, Ytest)\n",
    "                print(\"j / n_batches:\", j, \"/\", n_batches, \"cost:\", the_cost, \"error:\", error)\n",
    "                costs.append(the_cost)\n",
    "        plt.plot(costs)\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self, X):\n",
    "        return T.argmax(self.forward(X), axis=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        current_input = X\n",
    "        for ae in self.hidden_layers:\n",
    "            Z = ae.forward_hidden(current_input)\n",
    "            current_input = Z\n",
    "\n",
    "        # logistic layer\n",
    "        Y = T.nnet.softmax(T.dot(current_input, self.W) + self.b)\n",
    "        return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    Xtrain, Ytrain, Xtest, Ytest = getKaggleMNIST()\n",
    "    dnn = DNN([100,50])\n",
    "    dnn.fit(Xtrain, Ytrain, Xtest, Ytest, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training rbm 0\n",
      "epoch: 0\n",
      "batch 0 / 410 cost: 377.8018862514167\n",
      "batch 1 / 410 cost: 322.2913619891312\n",
      "batch 2 / 410 cost: 316.86395562244877\n",
      "batch 3 / 410 cost: 300.2166678422898\n",
      "batch 4 / 410 cost: 272.9640284695749\n",
      "batch 5 / 410 cost: 256.07768030433556\n",
      "batch 6 / 410 cost: 249.48524050698802\n",
      "batch 7 / 410 cost: 244.26591323649674\n",
      "batch 8 / 410 cost: 239.30241857811473\n",
      "batch 9 / 410 cost: 240.00179175094897\n",
      "batch 10 / 410 cost: 243.472201517157\n",
      "batch 11 / 410 cost: 239.6488202332834\n",
      "batch 12 / 410 cost: 256.0539567740851\n",
      "batch 13 / 410 cost: 227.73745025307255\n",
      "batch 14 / 410 cost: 224.7605999123447\n",
      "batch 15 / 410 cost: 226.4421805000808\n",
      "batch 16 / 410 cost: 234.15467290103786\n",
      "batch 17 / 410 cost: 222.13297107357366\n",
      "batch 18 / 410 cost: 220.8307018363832\n",
      "batch 19 / 410 cost: 223.6043515571457\n",
      "batch 20 / 410 cost: 235.39433556891157\n",
      "batch 21 / 410 cost: 211.0637591102951\n",
      "batch 22 / 410 cost: 209.06124432800624\n",
      "batch 23 / 410 cost: 206.72607562014662\n",
      "batch 24 / 410 cost: 206.32929694970875\n",
      "batch 25 / 410 cost: 203.80283410102643\n",
      "batch 26 / 410 cost: 202.1077938707266\n",
      "batch 27 / 410 cost: 200.2203047157968\n",
      "batch 28 / 410 cost: 198.48814590731712\n",
      "batch 29 / 410 cost: 196.8615748791544\n",
      "batch 30 / 410 cost: 195.57876525033674\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-58ca95c5b364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-df33a5ac2d1e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetKaggleMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-e366731dcd03>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, Xtest, Ytest, pretrain, learning_rate, mu, reg, epochs, batch_sz)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mcurrent_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mae\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpretrain_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# create current_input for the next layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-310c7b22fe66>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, learning_rate, epochs, batch_sz, show_fig)\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_sz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_sz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mtrain_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0mthe_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cost:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe_cost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mcosts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthe_cost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Blackbak/Applications/anaconda/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
